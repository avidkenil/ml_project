{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00025465d4725e87</td>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00031b1e95af7921</td>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00037261f536c51d</td>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00040093b2687caa</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "5  00025465d4725e87  \"\\n\\nCongratulations from me as well, use the ...      0   \n",
       "6  0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1   \n",
       "7  00031b1e95af7921  Your vandalism to the Matt Shirvington article...      0   \n",
       "8  00037261f536c51d  Sorry if the word 'nonsense' was offensive to ...      0   \n",
       "9  00040093b2687caa  alignment on this subject and which are contra...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  \n",
       "5             0        0       0       0              0  \n",
       "6             1        1       0       1              0  \n",
       "7             0        0       0       0              0  \n",
       "8             0        0       0       0              0  \n",
       "9             0        0       0       0              0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data['comment_text'] == ' ').any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               0\n",
       "comment_text     0\n",
       "toxic            0\n",
       "severe_toxic     0\n",
       "obscene          0\n",
       "threat           0\n",
       "insult           0\n",
       "identity_hate    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['comment_text'].fillna(value = '_na_',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary = [pair[0] for pair in word_counter.most_common(vocab_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_dim = len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_words = min(vocab_size+1, glove_dim)\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for i, word in enumerate(vocabulary):\n",
    "    if i >= vocab_size+1:\n",
    "        continue\n",
    "    embedding_vector = embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i+1] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.90863209, -1.83054233,  0.20075362, -0.09392044, -0.61499785,\n",
       "       -0.08694068, -0.53790446, -0.69105299,  0.24632631,  1.114145  ,\n",
       "        0.20596222,  0.1127703 ,  0.1055719 , -0.49976979,  0.16715639,\n",
       "       -0.61481833,  0.5863114 , -0.59645605, -0.90389832, -0.22466315,\n",
       "       -0.40626646, -0.65643533,  0.32985416, -0.09072985,  1.4024442 ,\n",
       "        0.79602253, -1.16494341, -0.12527208, -0.07377362,  0.02483354,\n",
       "        0.60662275,  0.07979284,  1.02550161, -0.02989132,  0.38192396,\n",
       "       -1.15426754, -0.14828249,  1.46536808,  0.11470578,  0.80675531,\n",
       "        0.05579536,  0.89502589, -0.87188143,  0.84338568,  1.14738712,\n",
       "        0.06312929, -0.85381121,  0.14439408, -0.15484467, -0.46886253])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx_to_word = {index+1:word for index,word in enumerate(vocabulary)}\n",
    "word_to_idx = {word:index+1 for index,word in enumerate(vocabulary)}\n",
    "idx_to_word[0] = '<pad>'\n",
    "word_to_idx['<pad>'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_seq(df):\n",
    "    all_tokens = [spacy_tokenizer(df[\"comment_text\"].iloc[i]) for i in range(df[\"comment_text\"].shape[0])]\n",
    "    seq_all = []\n",
    "    empty_indices = []\n",
    "    for i in range(df.shape[0]):\n",
    "        seq = []\n",
    "        for tok in all_tokens[i]:\n",
    "            tok_idx = word_to_idx.get(tok)\n",
    "            if tok_idx is None:\n",
    "                continue\n",
    "            seq.append(tok_idx)\n",
    "        if(len(seq) == 0):\n",
    "            empty_indices.append(i)\n",
    "            continue\n",
    "        seq_all.append(seq)\n",
    "        \n",
    "    target_df = df.loc[~df.index.isin(empty_indices), df.columns[2:]]\n",
    "    target = target_df.as_matrix()\n",
    "        #print(tok)\n",
    "    return seq_all,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_seq,target = make_seq(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(vectorized_seqs, seq_lengths):\n",
    "    max_seq_len = min(seq_lengths.max(),max_len)\n",
    "    seq_tensor = torch.zeros((len(vectorized_seqs), max_seq_len)).long()\n",
    "    for idx, (seq, seqlen) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "        min_seq_len = min(seqlen,max_len)\n",
    "        seq_tensor[idx, :min_seq_len] = torch.LongTensor(seq[:min_seq_len])\n",
    "    return seq_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_seq_lengths = torch.LongTensor([len(s) for s in all_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_seq_pad = pad_sequences(all_seq,all_seq_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_val,y_train,y_val = train_test_split(pd.DataFrame(all_seq_pad.numpy()),pd.DataFrame(target),test_size = 0.2,stratify = target[:,3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27607"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = torch.from_numpy(x_train.index.values).long()\n",
    "val_ids = torch.from_numpy(x_val.index.values).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq_lengths = torch.index_select(all_seq_lengths,0,train_ids)\n",
    "val_seq_lengths = torch.index_select(all_seq_lengths,0,val_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82583454281567492"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:,5].sum()/target[:,5].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ToxicDataset(Dataset):\n",
    "    def __init__(self,data_tensor,target_tensor,length_tensor):\n",
    "        assert data_tensor.size(0) == target_tensor.size(0) == length_tensor.size(0)\n",
    "        self.data_tensor = data_tensor\n",
    "        self.target_tensor = target_tensor\n",
    "        self.length_tensor = length_tensor\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return self.data_tensor[index],self.target_tensor[index],self.length_tensor[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data_tensor.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_t = torch.from_numpy(x_train.values).long()\n",
    "y_train_t = torch.from_numpy(y_train.values).long()\n",
    "train_dataset = ToxicDataset(x_train_t,y_train_t,train_seq_lengths)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True,num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_t = torch.from_numpy(x_val.values).long()\n",
    "y_val_t = torch.from_numpy(y_val.values).long()\n",
    "val_dataset = ToxicDataset(x_val_t,y_val_t,val_seq_lengths)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=2*batch_size,shuffle=False,num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_nn = nn.Embedding(vocab_size+1, embedding_dim=embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-9.0863e-01 -1.8305e+00  2.0075e-01  ...   1.4439e-01 -1.5484e-01 -4.6886e-01\n",
       " 4.9427e-01  1.3234e-01 -2.3199e-02  ...   2.4152e-01 -2.7658e-01 -3.7987e-01\n",
       " 1.2900e+00  5.2437e-02 -4.3540e-01  ...   4.4992e-02 -9.1239e-01 -1.0489e+00\n",
       "                ...                   â‹±                   ...                \n",
       "-2.7085e-01 -9.7770e-01 -9.7691e-01  ...   7.1800e-01  4.4413e-01 -4.4221e-01\n",
       " 1.1607e-01  7.2430e-01 -5.0700e-01  ...   3.4732e-01 -1.0340e+00 -6.5862e-01\n",
       " 1.1510e+00  8.3473e-02 -2.0625e-01  ...   1.2821e+00  5.4254e-02 -1.1215e-02\n",
       "[torch.FloatTensor of size 1001x50]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_nn.weight.data.copy_(torch.from_numpy(embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_nn.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTMBasicNet(nn.Module):\n",
    "    def __init__(self,input_size, output_size,hidden_size,num_layers,num_classes,embed):\n",
    "        super(LSTMBasicNet,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embed = embed\n",
    "        self.lstm = nn.LSTM(input_size=self.input_size,hidden_size=hidden_size,num_layers = num_layers,bidirectional = True, batch_first = True, dropout = 0.1)\n",
    "        self.fc1 = nn.Linear(in_features=hidden_size*2,out_features=output_size)\n",
    "        self.fc2 = nn.Linear(in_features=output_size,out_features=num_classes)\n",
    "        \n",
    "    def forward(self,x,lengths):\n",
    "        embed_out = self.embed(x)\n",
    "        packed_input = pack_padded_sequence(embed_out,batch_first=True,lengths=lengths) \n",
    "        h0 = Variable(torch.zeros(self.num_layers*2, x.size(0), self.hidden_size)) # 2 for bidirection \n",
    "        c0 = Variable(torch.zeros(self.num_layers*2, x.size(0), self.hidden_size))\n",
    "        packed_output,(hn,cn) = self.lstm(packed_input,(h0,c0))\n",
    "        lstm_out = pad_packed_sequence(packed_output,batch_first=True)\n",
    "        lstm_out = F.tanh(lstm_out)\n",
    "        fc1_out = F.dropout(F.relu(self.fc1(lstm_out[:,-1,:])),p=0.1)\n",
    "        out = F.sigmoid(self.fc2(fc1_out))\n",
    "        \n",
    "        return out\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMBasicNet(embed_size,64,64,2,6,embed_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "notify_time": "5"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
