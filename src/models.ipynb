{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "from os import path, makedirs\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, PredefinedSplit\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.base import clone\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost.sklearn import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(data_dict):\n",
    "    try:\n",
    "        print(json.dumps(data_dict, indent=4))\n",
    "    except TypeError:\n",
    "        print(data_dict)\n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path='../data/', is_clean=0, is_os=0):\n",
    "    print('Loading data...')\n",
    "    clean = '_clean' if is_clean else ''\n",
    "    os = '_os' if is_os else ''\n",
    "    data_sets = {}\n",
    "    data_cols = [\n",
    "        'data', 'X_train', 'X_val', 'X_train_val', 'X_test', \\\n",
    "        'y_train', 'y_val', 'y_train_val', 'y_test'\n",
    "    ]\n",
    "    \n",
    "    for i, col in enumerate(data_cols):\n",
    "        data_cols[i] = col + clean + os\n",
    "    \n",
    "    for col in data_cols:\n",
    "        data_sets[col] = pickle.load(open(data_path+'{}.pkl'.format(col),'rb'))\n",
    "    \n",
    "    return data_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_clean, is_os = 1, 0\n",
    "clean, os = '_clean', ''\n",
    "pickle_path = '../pickle_objects/'\n",
    "model_path = pickle_path + 'models/'\n",
    "\n",
    "data_sets = load_data('../data/', is_clean=1, is_os=0)\n",
    "\n",
    "target_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "vectorizers = ['countvec', 'tfidf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_sets['data'+clean+os].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_sets['X_train'+clean+os].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print value counts for each target\n",
    "for target in target_cols:\n",
    "    print('{}:'.format(target))\n",
    "    for data in data_sets:\n",
    "        if 'y' in data:\n",
    "            value_counts = data_sets[data][target].value_counts()\n",
    "            print('{}: {:.3f}%\\t'.format(data, 100*value_counts[1]/sum(value_counts)), end='')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feats = 1000\n",
    "ngrams = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ngrams(data_sets, data_col, num_feats, ngrams, pickle_path='../pickle_objects/', is_clean=1, is_os=0):\n",
    "    print('Loading ngrams...')\n",
    "    clean = '_clean' if is_clean else ''\n",
    "    os = '_os' if is_os else ''\n",
    "    data_col += clean+os\n",
    "    ngrams_data = {}\n",
    "    ngram_range = list(map(lambda x: x+1,range(ngrams)))\n",
    "    vec_params = {'analyzer': 'word', 'lowercase': True,'max_features': num_feats, 'ngram_range': ngram_range}\n",
    "    \n",
    "    for vec in vectorizers:\n",
    "        # Load vectorizer if present\n",
    "        file_name = '{}{}_ngrams_{}_{}_{}.pkl'.format(pickle_path, vec, data_col, num_feats, ngrams)\n",
    "        if path.isfile(file_name):\n",
    "            ngrams_data[vec] = pickle.load(open(file_name, 'rb'))\n",
    "        else:\n",
    "            # Fit, store, and load vectorizer\n",
    "            print('ngrams not found. Fitting and dumping them...')\n",
    "            ngrams_vec = CountVectorizer(**vec_params) if vec == 'countvec' else TfidfVectorizer(**vec_params)\n",
    "            ngrams_vec.fit(data_sets[data_col]['comment_text'])\n",
    "            ngrams_data[vec] = ngrams_vec\n",
    "            if not path.exists(pickle_path):\n",
    "                makedirs(pickle_path)\n",
    "            pickle.dump(ngrams_vec, open(file_name, 'wb'))\n",
    "    return ngrams_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_data = load_ngrams(data_sets, 'X_train', num_feats, ngrams, pickle_path, is_clean, is_os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_ngrams(data_set, data_cols, ngrams_data, vectorizers):\n",
    "    print('Transforming data to ngrams...')\n",
    "    for data in data_cols:\n",
    "        for vec in vectorizers:\n",
    "            data_sets[data+'_'+vec] = ngrams_data[vec].transform(data_sets[data]['comment_text'])\n",
    "    return data_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sets = transform_to_ngrams(data_sets, ['X_train_val'+clean+os], ngrams_data, vectorizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(X_train, X_test):\n",
    "    print('Normalizing data...')\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(base_model, X, y, param_grid, target_cols, scoring='roc_auc', cv=None):\n",
    "    if cv:\n",
    "        models, mean_val_scores, params = {}, [], []\n",
    "        for target in target_cols:\n",
    "            print('\\t\\tRunning for {}'.format(target))\n",
    "            model_target = GridSearchCV(base_model, param_grid, cv=cv, scoring=scoring, n_jobs=-1, refit=False)\n",
    "            model_target.fit(X, y[target])\n",
    "            mean_val_scores.append(model_target.cv_results_['mean_test_score'])\n",
    "            if not params:\n",
    "                params = model_target.cv_results_['params']\n",
    "        mean_val_scores = np.mean(np.array(mean_val_scores), axis=0)\n",
    "        best_param_idx = np.argmax(mean_val_scores)\n",
    "        models['best_params_'], models['best_mean_score_'] = \\\n",
    "        params[best_param_idx], mean_val_scores[best_param_idx]\n",
    "    else:\n",
    "        models = {}\n",
    "        for target in target_cols:\n",
    "            print('\\t\\tRunning for {}'.format(target))\n",
    "            model = clone(base_model)\n",
    "            model.set_params(**param_grid).fit(X, y[target])\n",
    "            models[target] = model\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_models(model, X, model_name, target_cols, model_path='../pickle_objects/models/', cv=None):\n",
    "    if not path.exists(model_path):\n",
    "        makedirs(model_path)\n",
    "    file_name = '{}{}_{}.pkl'.format(model_path, model_name, X)\n",
    "    if not path.isfile(file_name):\n",
    "        print('\\tDumping {}...'.format(model_name))\n",
    "        pickle.dump(model, open(file_name, 'wb'))\n",
    "    else:\n",
    "        print('\\tDid not dump {}: File already exists in \"{}\".'.format(model_name, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_all_models(data_sets, data_cols, model_list, param_grids, \\\n",
    "                   target_cols, model_path='../pickle_path/models/', cv=None):\n",
    "    best_models, best_params, best_scores = {}, {}, {}\n",
    "    X, y = data_cols\n",
    "    for model in model_list:\n",
    "        print('\\tRunning {}...'.format(model))\n",
    "        best_models[model] = fit_model(model_list[model], data_sets[X], \\\n",
    "                                          data_sets[y], param_grids[model], \\\n",
    "                                          target_cols, 'roc_auc', cv)\n",
    "        if cv:\n",
    "            pretty_print(best_models[model])\n",
    "        else:\n",
    "            dump_models(best_models[model], X, model, target_cols, model_path, cv)\n",
    "    return best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = {\n",
    "    'lrl1': LogisticRegression(penalty='l1'),\n",
    "    'lrl2': LogisticRegression(penalty='l2'),\n",
    "    'rf': RandomForestClassifier(),\n",
    "    'xgb': XGBClassifier(),\n",
    "#     'bnb': BernoulliNB(),\n",
    "#     'gnb': GaussianNB(),\n",
    "#     'svm': SVC(kernel='linear')\n",
    "}\n",
    "    \n",
    "param_grids = {\n",
    "    'lrl1': {'C': np.concatenate((np.reciprocal(np.arange(1., 13., 3.)), \\\n",
    "                                  np.logspace(1., 6., num=6, endpoint=True, base=10)))},\n",
    "    'lrl2': {'C': np.concatenate((np.reciprocal(np.arange(1., 13., 3.)), \\\n",
    "                                  np.logspace(1., 6., num=6, endpoint=True, base=10)))},\n",
    "    'rf': {\n",
    "        'n_estimators': np.arange(50, 250, 50),\n",
    "        'max_features': ['auto', 'log2'],\n",
    "        'max_depth': np.arange(3, 13, 2)\n",
    "    },\n",
    "    'xgb': {\n",
    "        'n_estimators': np.arange(50, 250, 50),\n",
    "        'max_depth': np.arange(3, 13, 2),\n",
    "        'learning_rate': [1e-1, 1e-3, 1e-5],\n",
    "        'reg_lambda': [1, 10, 1e-1]\n",
    "    },\n",
    "#     'bnb': {},\n",
    "#     'gnb': {},\n",
    "#     'svm': {'C': np.concatenate((np.arange(1, 13, 3), np.logspace(1, 6, num=6, endpoint=True, base=10)))},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set predefined split for CV\n",
    "# 0 corresponds to val, -1 to train\n",
    "val_fold = [-1]*len(data_sets['X_train'+clean+os]) + [0]*len(data_sets['X_val'+clean+os])\n",
    "predefined_split = PredefinedSplit(test_fold=val_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best hyperparameter settings for each data set for each model\n",
    "best_models = {}\n",
    "for vec in vectorizers:\n",
    "    print('Running for {}...'.format(vec))\n",
    "    data_cols = ('X_train_val'+clean+os+'_'+vec, 'y_train_val'+clean+os)\n",
    "    best_models[vec] = fit_all_models(data_sets, data_cols, model_list, param_grids, \\\n",
    "                                 target_cols, model_path, cv=predefined_split)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refit_best_model(data_sets, model_list, data_cols, best_models, \\\n",
    "                     target_cols, model_path='../pickle_path/models/'):\n",
    "    # Refit all models with all data sets with best hyperparameters\n",
    "    print('Refitting with best parameters...')\n",
    "    best_params = {}\n",
    "    for model in model_list:\n",
    "        best_params[model] = best_models[model]['best_params_']\n",
    "    best_refitted_models = fit_all_models(data_sets, data_cols, model_list, \\\n",
    "                                          best_params, target_cols, model_path, cv=None)\n",
    "    return best_refitted_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_data = load_ngrams(data_sets, 'X_train_val', num_feats, ngrams, pickle_path, is_clean, is_os)\n",
    "data_sets = transform_to_ngrams(data_sets, ['X_train_val'+clean+os, 'X_test'+clean+os], \\\n",
    "                                ngrams_data, vectorizers)\n",
    "best_refitted_models = {}\n",
    "for vec in vectorizers:\n",
    "    print('\\nRunning for {}...'.format(vec))\n",
    "    data_cols = ('X_train_val'+clean+os+'_'+vec, 'y_train_val'+clean+os)\n",
    "    best_refitted_models[vec] = refit_best_model(data_sets, model_list, data_cols, \\\n",
    "                                            best_models[vec], target_cols, model_path)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels_and_probas(fitted_models, model_list, X, target_cols):\n",
    "    probabilities = {}\n",
    "    for model in model_list:\n",
    "        probabilities[model] = {}\n",
    "        for target in target_cols:\n",
    "            probabilities[model][target] = fitted_models[model][target].predict_proba(X)[:,1]\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_roc_curves(y_test, probabilities, model_list, target_cols, vec='countvec', \\\n",
    "                          model_or_target='model', plots_path='../plots/'):\n",
    "    aucs = {}\n",
    "    # Plot by model\n",
    "    if model_or_target == 'model':\n",
    "        for model in model_list:\n",
    "            print('Plotting ROC curve for {}...'.format(model))\n",
    "            aucs[model] = {}\n",
    "            plt.figure(figsize=(10,8))\n",
    "            for target in target_cols:\n",
    "                fpr, tpr, threshold = roc_curve(y_test[target], probabilities[model][target])\n",
    "                auc_value = auc(fpr, tpr)\n",
    "                aucs[model][target] = auc_value\n",
    "                plt.plot(fpr, tpr, label='{}: {:0.5f}'.format('auc_'+target, auc_value))\n",
    "            plt.xlabel('fpr')\n",
    "            plt.ylabel('tpr')\n",
    "            plt.title('ROC Curve for {} with {}'.format(model, vec))\n",
    "            plt.xlim([0, 1])\n",
    "            plt.ylim([0, 1])\n",
    "            plt.legend(bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0.)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(plots_path+'roc_'+model+'_'+vec+'.jpg')\n",
    "            plt.close('all')\n",
    "    # Plot by target column\n",
    "    elif model_or_target == 'target':\n",
    "        for target in target_cols:\n",
    "            print('Plotting ROC curve for {}...'.format(target))\n",
    "            aucs[target] = {}\n",
    "            plt.figure(figsize=(10,8))\n",
    "            for model in model_list:\n",
    "                fpr, tpr, threshold = roc_curve(y_test[target], probabilities[model][target])\n",
    "                auc_value = auc(fpr, tpr)\n",
    "                aucs[target][model] = auc_value\n",
    "                plt.plot(fpr, tpr, label='{}: {:0.5f}'.format('auc_'+model, auc_value))\n",
    "            plt.xlabel('fpr')\n",
    "            plt.ylabel('tpr')\n",
    "            plt.title('ROC Curve for {} with {}'.format(target, vec))\n",
    "            plt.xlim([0, 1])\n",
    "            plt.ylim([0, 1])\n",
    "            plt.legend(bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0.)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(plots_path+'roc_'+target+'_'+vec+'.jpg')\n",
    "            plt.close('all')\n",
    "    else:\n",
    "        raise TypeError(\"Parameter 'model_or_target' must be one of 'model' or 'target'.\")\n",
    "    return aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_auc(aucs, model_list=None, target_cols=None, model_or_target='model'):\n",
    "    print('Computing mean AUCs...')\n",
    "    mean_aucs = {}\n",
    "    # Compute mean auc by model\n",
    "    columns = model_list if model_or_target == 'model' else target_cols\n",
    "    for col in columns:\n",
    "        mean_aucs[col] = np.mean(list(aucs[col].values()))\n",
    "    return mean_aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aucs_df(aucs, model_list, target_cols, model_or_target='model'):\n",
    "    print('Generating AUCs DataFrame...')\n",
    "    aucs_df = pd.DataFrame.from_dict(aucs)\n",
    "    aucs_df['mean'] = np.mean(aucs_df, axis=1)\n",
    "    aucs_df.loc['mean'] = np.mean(aucs_df, axis=0)\n",
    "    return aucs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_or_target = 'target'\n",
    "\n",
    "for vec in vectorizers:\n",
    "    probabilities = predict_labels_and_probas(best_refitted_models[vec], model_list, \\\n",
    "                                              data_sets['X_test'+clean+os+'_'+vec], target_cols)\n",
    "    \n",
    "    aucs = plot_model_roc_curves(data_sets['y_test'+clean+os], probabilities, \\\n",
    "                          model_list, target_cols, vec, model_or_target, '../plots/')\n",
    "\n",
    "    pretty_print(aucs)\n",
    "\n",
    "    mean_aucs = get_mean_auc(aucs, model_list, target_cols, model_or_target)\n",
    "\n",
    "    pretty_print(mean_aucs)\n",
    "\n",
    "    aucs_df = get_aucs_df(aucs, model_list, target_cols, model_or_target)\n",
    "    \n",
    "    print('AUCs DataFrame for {}:'.format(vec))\n",
    "    print(aucs_df)\n",
    "\n",
    "    print('Dumping AUCs DataFrame... ', end='', flush=True)\n",
    "    pickle.dump(aucs_df, open('{}aucs_{}.pkl'.format(pickle_path, vec), 'wb'))\n",
    "    print('Done.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "notify_time": "5"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
