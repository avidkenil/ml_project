{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "import string\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_data(data_sets, data_path='../data/', force=False):\n",
    "    if not os.path.exists(data_path):\n",
    "        print('Creating {}... '.format(data_path), end='', flush=True)\n",
    "        os.makedirs(data_path)\n",
    "        print('Done.')\n",
    "    for data in data_sets:\n",
    "        file_path = data_path+'{}.pkl'.format(data)\n",
    "        if force or not os.path.isfile(file_path):\n",
    "            print('Dumping {}... '.format(file_path), end='', flush=True)\n",
    "            pickle.dump(data_sets[data], open(file_path, 'wb'))\n",
    "            print('Done.')\n",
    "        else:\n",
    "            print('Did not dump {}: File already exists.'.format(file_path))\n",
    "\n",
    "def split_data(data, target_cols, clean='_clean'):\n",
    "    # Split data into train (0.8), validation (0.2), and test (0.2) sets\n",
    "    X_train_val, X_test, y_train_val, y_test = \\\n",
    "    train_test_split(data.drop(target_cols, axis=1), data[target_cols], test_size=0.2, random_state=1337)\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = \\\n",
    "    train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=1337)\n",
    "    \n",
    "    # Order (separate) train+val set\n",
    "    train_idx, val_idx = X_train.index.values, X_val.index.values\n",
    "    X_train_val = X_train_val.loc[train_idx].append(X_train_val.loc[val_idx])\n",
    "    y_train_val = y_train_val.loc[train_idx].append(y_train_val.loc[val_idx])\n",
    "    \n",
    "    data_sets = {\n",
    "        'X_train': X_train,\n",
    "        'X_val': X_val,\n",
    "        'X_train_val': X_train_val,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_val': y_val,\n",
    "        'y_train_val': y_train_val,\n",
    "        'y_test': y_test\n",
    "    }\n",
    "    \n",
    "    data_sets = {data+clean: data_sets[data] for data in data_sets}\n",
    "\n",
    "    return data_sets\n",
    "\n",
    "def oversample_data(data_sets, oversampling_cols, oversampling_target_cols, clean='_clean'):\n",
    "    for (X, y) in oversampling_cols:\n",
    "        X, y = X+clean, y+clean\n",
    "        X_os, y_os = X+'_os', y+'_os'\n",
    "        total_rows = data_sets[y].shape[0]\n",
    "        oversampling_indices = []\n",
    "        for col in oversampling_target_cols:\n",
    "            count = data_sets[y][data_sets[y][col] == 1].shape[0]\n",
    "            while count/total_rows < 0.05:\n",
    "                indices_1 = data_sets[y][data_sets[y][col] == 1].index.values\n",
    "                rand_index = np.random.choice(indices_1)\n",
    "                oversampling_indices.append(rand_index)\n",
    "                count += 1\n",
    "        \n",
    "        data_sets[X_os] = data_sets[X].append(data_sets[X].loc[oversampling_indices])\n",
    "        data_sets[y_os] = data_sets[y].append(data_sets[X].loc[oversampling_indices])\n",
    "        data_sets[X_os], data_sets[y_os] = shuffle(data_sets[X_os], data_sets[y_os])\n",
    "    \n",
    "    return data_sets\n",
    "\n",
    "def prepare_stopwords():\n",
    "    NEGATE = [\"aint\", \"arent\", \"cannot\", \"cant\", \"couldnt\", \"darent\", \"didnt\", \"doesnt\",\n",
    "     \"ain't\", \"aren't\", \"can't\", \"couldn't\", \"daren't\", \"didn't\", \"doesn't\",\n",
    "     \"dont\", \"hadnt\", \"hasnt\", \"havent\", \"isnt\", \"mightnt\", \"mustnt\", \"neither\",\n",
    "     \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\",\n",
    "     \"neednt\", \"needn't\", \"never\", \"none\", \"nope\", \"nor\", \"not\", \"nothing\", \"nowhere\",\"no\",\n",
    "     \"oughtnt\", \"shant\", \"shouldnt\", \"uhuh\", \"wasnt\", \"werent\",\n",
    "     \"oughtn't\", \"shan't\", \"shouldn't\", \"uh-uh\", \"wasn't\", \"weren't\",\n",
    "     \"without\", \"wont\", \"wouldnt\", \"won't\", \"wouldn't\", \"rarely\", \"seldom\", \"despite\"]\n",
    "\n",
    "    stopwords = STOP_WORDS.copy()\n",
    "    for word in STOP_WORDS:\n",
    "        if word in NEGATE:\n",
    "            stopwords.remove(word)\n",
    "\n",
    "    return stopwords\n",
    "\n",
    "def spacy_tokenizer(parser, sentence, stopwords, punctuations):\n",
    "    tokens = parser(sentence)\n",
    "    tokens = [tok.lemma_.lower().strip() for tok in tokens]\n",
    "    tokens = [tok for tok in tokens if (tok not in stopwords and tok not in punctuations)]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def clean_data(data, col, parser, stopwords, punctuations):\n",
    "    clean_text = []\n",
    "    for text in data[col]:\n",
    "        clean_text.append(spacy_tokenizer(parser, text, stopwords, punctuations))\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print('Loading data... ', end='', flush=True)\n",
    "data_path = '../data/'\n",
    "data = pd.read_csv(data_path+'data.csv')\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print value counts for all target classes\n",
    "target_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "for col in target_cols:\n",
    "    print(data[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameter indicating processing for uncleaned data\n",
    "clean = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "print('Splitting raw data into train-val-test sets... ', end='', flush=True)\n",
    "data_sets = split_data(data, target_cols, clean)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversample data\n",
    "print('Oversampling raw data... ', end='', flush=True)\n",
    "oversampling_target_cols = ['severe_toxic', 'threat', 'identity_hate']\n",
    "data_sets = oversample_data(data_sets, [('X_train', 'y_train'), ('X_train_val', 'y_train_val')], \\\n",
    "                            oversampling_target_cols, clean)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump data\n",
    "dump_data(data_sets, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All preprocessing that follows is for cleaned data\n",
    "clean = '_clean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spacy parser and punctuations\n",
    "spacy_en = spacy.load('en')\n",
    "parser = English()\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Remove negations from stopword list\n",
    "print('Preparing stopwords... ', end='', flush=True)\n",
    "stopwords = prepare_stopwords()\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data\n",
    "print('Cleaning stopwords... ', end='', flush=True)\n",
    "data['comment_text'] = clean_data(data, 'comment_text', parser, stopwords, punctuations)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump cleaned data\n",
    "dump_data({'data_clean': data}, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split cleaned data\n",
    "print('Splitting cleaned data into train-val-test sets... ', end='', flush=True)\n",
    "data_sets = split_data(data, target_cols, clean)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversample cleaned data\n",
    "print('Oversampling cleaned data... ', end='', flush=True)\n",
    "oversampling_target_cols = ['severe_toxic', 'threat', 'identity_hate']\n",
    "data_sets = oversample_data(data_sets, [('X_train', 'y_train'), ('X_train_val', 'y_train_val')], \\\n",
    "                            oversampling_target_cols, clean)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump data\n",
    "dump_data(data_sets, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
